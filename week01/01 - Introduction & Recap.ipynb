{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction & Recap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "\n",
    "- データコンペに関する説明\n",
    "- このコースの対象はMLの基礎事項を理解していることを前提としている。\n",
    "    - 具体的には、python, sklearn, MLモデルの学習についてある程度知っている。\n",
    "- 様々なプロセスで効率的に取り組めるようになるために、ある種の「直感」を養うことが必要。\n",
    "- コンペに時間を捧げるほど、より多くのことをを学ぶことができる。\n",
    "- このコースを提供しているのは、Kaggle 等のデータコンペに熱心に取り組んできた人物ら（ロシア・Yandex）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コース概要\n",
    "\n",
    "### スケジュール\n",
    "\n",
    "- **WEEK1: Starting Point**\n",
    "    - データコンペとRecapについて\n",
    "    - 特徴量の前処理と抽出\n",
    "- **WEEK2: Data Pipeline**\n",
    "    - EDA\n",
    "    - Validation\n",
    "    - Data Leaks\n",
    "- **WEEK3: Improvement**\n",
    "    - Metrics\n",
    "    - Mean-encodings\n",
    "- **WEEK4: Advanced**\n",
    "    - Advenced features\n",
    "    - Hyperparameter optimization\n",
    "    - Ensembles\n",
    "- **WEEK5: Wrapping Up**\n",
    "    - Final project\n",
    "    - Winning solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データコンペについて\n",
    "\n",
    "### 概要\n",
    "\n",
    "- ページ説明\n",
    "    - Data\n",
    "    - Model\n",
    "    - Submission\n",
    "    - Evaluation\n",
    "    - Leaderboard\n",
    "\n",
    "- プラットフォーム\n",
    "    - Kaggle\n",
    "    - DrivenData\n",
    "    - CrowdAnalityx\n",
    "    - CodaLab\n",
    "    - DataScienceChallenge.net\n",
    "    - Datascience.net\n",
    "    - Signle-competition sites(like KDD, VizDooM)\n",
    "    \n",
    "- 参加すべき理由\n",
    "    - 学習とネットワーキングの機会\n",
    "    - 様々なアプローチを試せる\n",
    "    - コミュニティに入れる\n",
    "    - 運が良ければ賞金を得られる\n",
    "    \n",
    "### 実世界とデータコンペの違い\n",
    "\n",
    "- 実世界\n",
    "    - ビジネス理解\n",
    "    - 問題の定式化\n",
    "    - データ収集\n",
    "    - データ前処理\n",
    "    - モデリング\n",
    "    - 評価指標を決定\n",
    "    - デプロイ\n",
    "- データコンペ\n",
    "    - （ビジネス理解）\n",
    "    - （データ収集）\n",
    "    - データ前処理\n",
    "    - モデリング\n",
    "    \n",
    "- データコンペはアルゴリズムのみではない\n",
    "    - 既存のアプローチをチューニングする必要がある\n",
    "        - 既存ライブラリのソースコードをいじったりすることも必要になりうる。\n",
    "    - 洞察が必要\n",
    "    - 時には ML 以外のことが必要になる\n",
    "        - ヒューリスティック・手動のデータ分析もOK\n",
    "        - 複雑な解放・先進的な特徴エンジニアリング・膨大な計算も避けなければならないわけではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### ML アルゴリズムの系統\n",
    "\n",
    "#### Linear\n",
    "\n",
    "- アルゴリズム例\n",
    "    - Logistic Regression\n",
    "    - SVM\n",
    "- 特徴\n",
    "    - 線形モデルはスパースな高次元データに対して有効\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "    - vowpal-rabbit\n",
    "\n",
    "#### Tree-based\n",
    "\n",
    "- アルゴリズム例\n",
    "    - Desicion Tree\n",
    "    - Random Forest\n",
    "    - GBDT\n",
    "- 特徴\n",
    "    - 決定木の組み合わせによって表現力が高く、テーブルデータに対して有効\n",
    "    - 線形従属（斜めの線?）を捉えるのは苦手である。\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "    - dmlc/XGBoost\n",
    "    - Microsoft/LightGBM\n",
    "    \n",
    "#### kNN\n",
    "\n",
    "- 特徴\n",
    "    - 距離が近いものが似たラベルを持つという発想。\n",
    "    - シンプルな方法だが、近傍法に基づく特徴量は有益なことがしばしばある。\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "- 特徴\n",
    "    - 決定木系と比べて滑らかな分離カーブを表現できる\n",
    "    - 画像・音声・テキスト・文章に特に有効なことがある。\n",
    "- フレームワーク\n",
    "    - TensorFlow\n",
    "    - Keras\n",
    "    - MXNet\n",
    "    - PyTorch\n",
    "    - Lasagne\n",
    "    \n",
    "### No Free Lunch Theorem\n",
    "\n",
    "- 全てのタスクにおいて他の全てのモデルを凌ぐモデルは存在しないということ。\n",
    "- そのため様々なモデルに精通している必要がある。\n",
    "\n",
    "### Scikit-Learn の分離器比較\n",
    "\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "- 様々な分離器の決定境界を概観できる。\n",
    "- なぜそのような決定境界ができるのか、その理由を直感的に理解できることが推奨される。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "- 「銀の弾丸」のアルゴリズムは存在しない\n",
    "- 線形モデルは空間を2つの小空間に分ける\n",
    "- ツリーベースモデルは空間を箱に分ける\n",
    "- kNNモデルはどのように各点の「近さ」を定義するかに大きく依存する\n",
    "- 順伝播ニューラルネットは滑らかな非線形の決定境界を生成する\n",
    "- 最も強力な手法は GDBT と NN だが、線形モデルやkNNも時にそれらを凌ぐことがあるので過小評価してはならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハードウェア/ソフトウェア要件\n",
    "\n",
    "### ハードウェア\n",
    "\n",
    "- 多くのコンペ（画像系を除く）は以下のもので十分\n",
    "    - 高性能ノートPC\n",
    "    - メモリ16GB\n",
    "    - 4コア\n",
    "- 以下のものがあるとより良い\n",
    "    - デスクトップPC\n",
    "    - メモリ32GB\n",
    "    - 6コア\n",
    "    \n",
    "- 重要なポイント\n",
    "    - メモリ： いろいろ捗る\n",
    "    - コア： より多くのことを、より早く試せる\n",
    "    - ストレージ： 画像や容量の大きいデータをミニバッチでアレコレする際に重要\n",
    "    \n",
    "- クラウドリソース\n",
    "    - コンピュータリソースが捗る\n",
    "        - AWS: スポットインスタンスはコストを抑えるが停止させられるリスクが有る\n",
    "        - MS Azure\n",
    "        - GCP\n",
    "        \n",
    "### ソフトウェア\n",
    "\n",
    "- Python が多く用いられる\n",
    "    - データサイエンスに適したライブラリ: NumPy, pandas, Scikit-learn, matplotlib\n",
    "    - IDE: IPython, jupyter\n",
    "    - Special packages: dmlc/XGBoost, Microsoft/LightGBM, Keras, danielfrg/tsne\n",
    "    - External tools: VOWPAL WABBIT, srendle/libfm, gusetwalk/libffm, baidu/fast_rgf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リンク\n",
    "\n",
    "### クラウド\n",
    "\n",
    "- [AWS](https://aws.amazon.com/jp/), [Google Cloud](https://cloud.google.com/), [Microsoft Azure](https://azure.microsoft.com/ja-jp/)\n",
    "\n",
    "### AWS スポットオプション\n",
    "- [Overview of Spot mechanism](https://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/using-spot-instances.html)\n",
    "- [Spot Setup Guide](https://datasciencebowl.com/aws_guide/)\n",
    "\n",
    "### スタック/パッケージ\n",
    "\n",
    "- [Basic SciPy stack (ipython, numpy, pandas, matplotlib)](https://www.scipy.org/)\n",
    "- [jupyter Notebook](https://jupyter.org/)\n",
    "- [Stand-alone python tSNE package](https://github.com/danielfrg/tsne)\n",
    "- スパースなクリック率のような(CTR-like)データで機能するライブラリ: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)\n",
    "- 別のツリーベースモデル: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))\n",
    "- データサイエンスに関する諸々のパッケージが入った Python ディストリビューション: [Anaconda](https://www.continuum.io/what-is-anaconda)\n",
    "- [Blog \"datas-frame\" (contains posts about effective Pandas usage)](https://tomaugspurger.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Preprocessing and Generation with Respect to Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "\n",
    "### 前処理と特徴量生成について\n",
    "\n",
    "前処理も特徴量生成もどのように処理するかは用いるモデルタイプに依存する。\n",
    "以下、具体例を示す。\n",
    "\n",
    "- カテゴリカル変数の前処理\n",
    "    - 線形モデルは One-hot Encoding などが有効\n",
    "    - ツリーベースモデルでは不要\n",
    "    \n",
    "- トレンドを反映させるための特徴量生成\n",
    "    - 線形モデルでは前週の売上を特徴量として足す\n",
    "    - ツリーベースでは Window で Target Encoding したものを特徴量として足す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数値変数（Numerical Features）\n",
    "\n",
    "### 前処理\n",
    "\n",
    "#### スケーリング（Scaling）\n",
    "\n",
    "- ツリーベースモデル\n",
    "    - 値のスケールに影響されない\n",
    "- 非ツリーベースモデル\n",
    "    - 値のスケールに影響を受ける\n",
    "    - kNN の例にして考えるとスケールが予測精度に与える影響を理解しやすい\n",
    "    \n",
    "- 具体的な手法\n",
    "    - sklearn.preprocessing.MinMaxScaler()\n",
    "        - 指定した最小値と最大値の範囲に値をスケーリングする（デフォルトでは0~1の範囲）\n",
    "    - sklearn.preprocessing.StandardScaler()\n",
    "        - mean を引いて std で割る（平均0・標準偏差を1にする）\n",
    "        \n",
    "- kNN においてスケーリングの考え方を応用して、より重要だと思われる特徴量のスケールを大きくすることで、モデルへのその特徴量の影響度合いを（意図的に）大きくすることができる。\n",
    "\n",
    "#### 外れ値（Outliers）\n",
    "\n",
    "- 線形モデルは外れ値に影響を受けやすい\n",
    "- これを避けるためのアプローチがいくつかある。\n",
    "    - 1. パーセンタイルをもとに値をクリッピングする\n",
    "    - 2. ランク変換（Rank Transformation）を行う。値のマッピングを行うイメージか。\n",
    "        - ```rank([-100, 0, 1e5])==[0, 1, 2]```  \n",
    "          ```rank([1000, 1, 10]) = [2, 0, 1]```  \n",
    "        \n",
    "#### その他\n",
    "\n",
    "- 非ツリーベースモデル（特にNN）でしばしば有効な前処理\n",
    "    - ログ変換（Log Transform): np.log(1+x)\n",
    "    - 指数変換(Rasing to the power<1): np.sqrt(x + 2/3)\n",
    "- これらは過分散を抑制する一方で、0に近い値の差を大きくする\n",
    "- 様々な前処理を行って学習を行ったモデルを組み合わせることはとても有効になりうる。\n",
    "\n",
    "### 特徴量生成\n",
    "\n",
    "- 特徴量生成とは\n",
    "    - 特徴量やタスクに関する知識を利用して新しい特徴量を作り出すこと。\n",
    "    - モデルの学習をよりシンプルかつ効果的にするのに役立つ。\n",
    "    - 事前知識や EDA を用いたり、仮説をもとに検証したり、あるいは直感によって新しい特徴量を引き出す。\n",
    "    \n",
    "- 数値変数に関する特徴量生成について\n",
    "    - 加減乗除・交互作用による特徴量生成は線形モデルのみならずツリーベースモデルに対しても有効である。\n",
    "    - ツリーベースモデルに掛け算や割り算による良い特徴量を追加することで、よりロバストで木の数を抑えたモデルを作ることができる。\n",
    "    - 直感に基づくザックリした特徴量も有効になりうる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリカル・順序変数（Categorical and Oridinal Features）\n",
    "\n",
    "### 前処理\n",
    "\n",
    "- カテゴリカル変数の中でも、数値の順序に意味があるものを順序変数（あるいは Order Categorical Features）と呼ぶ。\n",
    "\n",
    "#### Label Encoding\n",
    "- 順序変数を数値に変換する手法\n",
    "    - ツリーベースモデルでは特徴量を分割できるため有用\n",
    "    - 一方で、非ツリーベースモデルでは有効には働かない。\n",
    "- 実装方法\n",
    "    - sklearn.preprocessing.LabelEncoder: アルファベット順にソートされ変換される\n",
    "    - pandas.factorize: 出現順に順次変換される\n",
    "    \n",
    "#### Frequency Encoding\n",
    "- カテゴリの出現頻度に応じて値をマッピングする\n",
    "- ```encoding = titanic.groupby('Embarked').size()```  \n",
    "  ```encoding = encoding/len(titanic)```  \n",
    "  ```titanic['enc'] = titanic.Emberked.map(encoding)```  \n",
    "- これは値の分布に関する情報を持つので、線形モデルやツリーモデルで有効に働きうる。\n",
    "    - 線形モデルにとっては、もしカテゴリの頻度が目的変数と相関があれば有効になる\n",
    "    - ツリーベースモデルにとっても、同様の理由でより少ない分割で済むようになりうる。\n",
    "- 複数のカテゴリが同じ頻度で出現する場合、この方法ではそれらを区別することができない。\n",
    "    - scipy.stats.rankdata で対処できうる\n",
    "        \n",
    "#### One-Hot Encoding\n",
    "- カテゴリごとに列を作成し、それぞれのカテゴリについて1/0の値で埋める。\n",
    "- 長所\n",
    "    - 非ツリーベースモデルで有効に働く\n",
    "    - 変換後すでにスケーリングされている。\n",
    "- 短所\n",
    "    - ツリーベースモデルでは学習の時間を冗長にし、効果が出にくい\n",
    "- ちなみに、ツリーベースモデルについては、メモリを食うのでスパース行列にして持ったほうが良い。\n",
    "    - XGBoost や LightGBM ではスパース行列でカテゴリカル変数を保持するように指定できる。\n",
    "\n",
    "\n",
    "### 特徴量生成\n",
    "\n",
    "#### 特徴量の交互作用\n",
    "\n",
    "- 非ツリーベースモデルに対して有効。\n",
    "- 複数のカテゴリ特徴量の組み合わせを One-Hot の形で持つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 日付変数と座標変数（Datetime and Coordinates）\n",
    "\n",
    "### Date and Time\n",
    "\n",
    "1. 周期性\n",
    "    - 曜日・週・月・年・時間・分・秒 などとして利用できる\n",
    "2. Time since\n",
    "    - ある日からの経過時間 (Independent)\n",
    "    - 決まったある日時から/までの時間 (dependent)\n",
    "3. Difference between dates\n",
    "    - ある日時から別のある日時までの日時の差\n",
    "         - churn 予測における、date_diff（購買と電話の日時の差）\n",
    "         \n",
    "### Coordinate\n",
    "\n",
    "- ある地点からの距離\n",
    "- 集計された統計値を計算する\n",
    "- ツリーベースモデルを使用するとき、45度（あるいは22.5度）回転させると、決定境界を作らせやすくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値処理（Handling Missing Values）\n",
    "\n",
    "- 数値変数の欠損値には、NaN, Null, -1, 99 などがある。\n",
    "- ヒストグラムを見ることで、値が欠損値が置き換えられたものだと想定することができる。\n",
    "\n",
    "### 欠損値へのアプローチ\n",
    "\n",
    "#### 1. 通常の値の分布外にある値で置き換える（-999, -1, etc.）\n",
    "- 線形ネットワークモデルには適さない。\n",
    "\n",
    "#### 2. 平均・中央値で置き換える\n",
    "- 線形モデルやNNにとっては有効である。\n",
    "- ツリーベースモデルには適さない。\n",
    "\n",
    "#### 3. 値を「再構築」する\n",
    "- 例えば時系列データの場合、欠損値を予測して置き換えることができうる。\n",
    "- ただし、このようなことができる場合は多くない。\n",
    "\n",
    "\n",
    "### 欠損値に関する特徴量生成\n",
    "\n",
    "- 欠損値であるかどうかを示すバイナリの特徴量が作れる\n",
    "    - ツリーベースモデルやNNで有効（平均や中央値で置き換えるより）\n",
    "    - ただし、二重に列をつくることにもなる\n",
    "    \n",
    "- 欠損値を含む特徴量から別の特徴量を生成する際には、欠損値の置換に特に注意を払う必要がある。\n",
    "- また 数値変数の欠損を -999 などに補完してからTarget-Encoding などを行ってしまうと、補完した値に平均が引っ張られてしまう。\n",
    "    - このようなときは欠損値を無視して平均を取ったほうが良い。\n",
    "\n",
    "- XGBoost は欠損値をそのまま扱え、スコアを大幅に改善することがある。\n",
    "\n",
    "- 欠損値を異常値として扱うことができる。\n",
    "\n",
    "- 学習データに現れないカテゴリカル変数のカテゴリはモデルに欠損値として扱われるので、それは Frequency Encoding で置換して対処する方法が考えられる。\n",
    "    - 学習データとテストデータの両方を見て、その頻度を数え上げる。カテゴリの頻度と目的変数に依存関係があれば良い感じになる。\n",
    "        - データリークを引き起こすのでは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リンク\n",
    "\n",
    "### Feature preprocessing\n",
    "\n",
    "- [Preprocessing in Sklearn](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Andrew NG about gradient descent and feature scaling\n",
    "](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)\n",
    "- [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n",
    "### Feature generation\n",
    "\n",
    "- [Discover Feature Engineering, How to Engineer Features and How to Get Good at It\n",
    "](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "- [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
