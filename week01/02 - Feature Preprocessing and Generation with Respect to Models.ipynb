{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Preprocessing and Generation with Respect to Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "\n",
    "### 前処理と特徴量生成について\n",
    "\n",
    "前処理も特徴量生成もどのように処理するかは用いるモデルタイプに依存する。\n",
    "以下、具体例を示す。\n",
    "\n",
    "- カテゴリカル変数の前処理\n",
    "    - 線形モデルは One-hot Encoding などが有効\n",
    "    - ツリーベースモデルでは不要\n",
    "    \n",
    "- トレンドを反映させるための特徴量生成\n",
    "    - 線形モデルでは前週の売上を特徴量として足す\n",
    "    - ツリーベースでは Window で Target Encoding したものを特徴量として足す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数値変数（Numerical Features）\n",
    "\n",
    "### 前処理\n",
    "\n",
    "#### スケーリング（Scaling）\n",
    "\n",
    "- ツリーベースモデル\n",
    "    - 値のスケールに影響されない\n",
    "- 非ツリーベースモデル\n",
    "    - 値のスケールに影響を受ける\n",
    "    - kNN の例にして考えるとスケールが予測精度に与える影響を理解しやすい\n",
    "    \n",
    "- 具体的な手法\n",
    "    - sklearn.preprocessing.MinMaxScaler()\n",
    "        - 指定した最小値と最大値の範囲に値をスケーリングする（デフォルトでは0~1の範囲）\n",
    "    - sklearn.preprocessing.StandardScaler()\n",
    "        - mean を引いて std で割る（平均0・標準偏差を1にする）\n",
    "        \n",
    "- kNN においてスケーリングの考え方を応用して、より重要だと思われる特徴量のスケールを大きくすることで、モデルへのその特徴量の影響度合いを（意図的に）大きくすることができる。\n",
    "\n",
    "#### 外れ値（Outliers）\n",
    "\n",
    "- 線形モデルは外れ値に影響を受けやすい\n",
    "- これを避けるためのアプローチがいくつかある。\n",
    "    - 1. パーセンタイルをもとに値をクリッピングする\n",
    "    - 2. ランク変換（Rank Transformation）を行う。値のマッピングを行うイメージか。\n",
    "        - ```rank([-100, 0, 1e5])==[0, 1, 2]```  \n",
    "          ```rank([1000, 1, 10]) = [2, 0, 1]```  \n",
    "        \n",
    "#### その他\n",
    "\n",
    "- 非ツリーベースモデル（特にNN）でしばしば有効な前処理\n",
    "    - ログ変換（Log Transform): np.log(1+x)\n",
    "    - 指数変換(Rasing to the power<1): np.sqrt(x + 2/3)\n",
    "- これらは過分散を抑制する一方で、0に近い値の差を大きくする\n",
    "- 様々な前処理を行って学習を行ったモデルを組み合わせることはとても有効になりうる。\n",
    "\n",
    "### 特徴量生成\n",
    "\n",
    "- 特徴量生成とは\n",
    "    - 特徴量やタスクに関する知識を利用して新しい特徴量を作り出すこと。\n",
    "    - モデルの学習をよりシンプルかつ効果的にするのに役立つ。\n",
    "    - 事前知識や EDA を用いたり、仮説をもとに検証したり、あるいは直感によって新しい特徴量を引き出す。\n",
    "    \n",
    "- 数値変数に関する特徴量生成について\n",
    "    - 加減乗除・交互作用による特徴量生成は線形モデルのみならずツリーベースモデルに対しても有効である。\n",
    "    - ツリーベースモデルに掛け算や割り算による良い特徴量を追加することで、よりロバストで木の数を抑えたモデルを作ることができる。\n",
    "    - 直感に基づくザックリした特徴量も有効になりうる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリカル・順序変数（Categorical and Oridinal Features）\n",
    "\n",
    "### 前処理\n",
    "\n",
    "- カテゴリカル変数の中でも、数値の順序に意味があるものを順序変数（あるいは Order Categorical Features）と呼ぶ。\n",
    "\n",
    "#### Label Encoding\n",
    "- 順序変数を数値に変換する手法\n",
    "    - ツリーベースモデルでは特徴量を分割できるため有用\n",
    "    - 一方で、非ツリーベースモデルでは有効には働かない。\n",
    "- 実装方法\n",
    "    - sklearn.preprocessing.LabelEncoder: アルファベット順にソートされ変換される\n",
    "    - pandas.factorize: 出現順に順次変換される\n",
    "    \n",
    "#### Frequency Encoding\n",
    "- カテゴリの出現頻度に応じて値をマッピングする\n",
    "- ```encoding = titanic.groupby('Embarked').size()```  \n",
    "  ```encoding = encoding/len(titanic)```  \n",
    "  ```titanic['enc'] = titanic.Emberked.map(encoding)```  \n",
    "- これは値の分布に関する情報を持つので、線形モデルやツリーモデルで有効に働きうる。\n",
    "    - 線形モデルにとっては、もしカテゴリの頻度が目的変数と相関があれば有効になる\n",
    "    - ツリーベースモデルにとっても、同様の理由でより少ない分割で済むようになりうる。\n",
    "- 複数のカテゴリが同じ頻度で出現する場合、この方法ではそれらを区別することができない。\n",
    "    - scipy.stats.rankdata で対処できうる\n",
    "        \n",
    "#### One-Hot Encoding\n",
    "- カテゴリごとに列を作成し、それぞれのカテゴリについて1/0の値で埋める。\n",
    "- 長所\n",
    "    - 非ツリーベースモデルで有効に働く\n",
    "    - 変換後すでにスケーリングされている。\n",
    "- 短所\n",
    "    - ツリーベースモデルでは学習の時間を冗長にし、効果が出にくい\n",
    "- ちなみに、ツリーベースモデルについては、メモリを食うのでスパース行列にして持ったほうが良い。\n",
    "    - XGBoost や LightGBM ではスパース行列でカテゴリカル変数を保持するように指定できる。\n",
    "\n",
    "\n",
    "### 特徴量生成\n",
    "\n",
    "#### 特徴量の交互作用\n",
    "\n",
    "- 非ツリーベースモデルに対して有効。\n",
    "- 複数のカテゴリ特徴量の組み合わせを One-Hot の形で持つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 日付変数と座標変数（Datetime and Coordinates）\n",
    "\n",
    "### Date and Time\n",
    "\n",
    "1. 周期性\n",
    "    - 曜日・週・月・年・時間・分・秒 などとして利用できる\n",
    "2. Time since\n",
    "    - ある日からの経過時間 (Independent)\n",
    "    - 決まったある日時から/までの時間 (dependent)\n",
    "3. Difference between dates\n",
    "    - ある日時から別のある日時までの日時の差\n",
    "         - churn 予測における、date_diff（購買と電話の日時の差）\n",
    "         \n",
    "### Coordinate\n",
    "\n",
    "- ある地点からの距離\n",
    "- 集計された統計値を計算する\n",
    "- ツリーベースモデルを使用するとき、45度（あるいは22.5度）回転させると、決定境界を作らせやすくなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠損値処理（Handling Missing Values）\n",
    "\n",
    "- 数値変数の欠損値には、NaN, Null, -1, 99 などがある。\n",
    "- ヒストグラムを見ることで、値が欠損値が置き換えられたものだと想定することができる。\n",
    "\n",
    "### 欠損値へのアプローチ\n",
    "\n",
    "#### 1. 通常の値の分布外にある値で置き換える（-999, -1, etc.）\n",
    "- 線形ネットワークモデルには適さない。\n",
    "\n",
    "#### 2. 平均・中央値で置き換える\n",
    "- 線形モデルやNNにとっては有効である。\n",
    "- ツリーベースモデルには適さない。\n",
    "\n",
    "#### 3. 値を「再構築」する\n",
    "- 例えば時系列データの場合、欠損値を予測して置き換えることができうる。\n",
    "- ただし、このようなことができる場合は多くない。\n",
    "\n",
    "\n",
    "### 欠損値に関する特徴量生成\n",
    "\n",
    "- 欠損値であるかどうかを示すバイナリの特徴量が作れる\n",
    "    - ツリーベースモデルやNNで有効（平均や中央値で置き換えるより）\n",
    "    - ただし、二重に列をつくることにもなる\n",
    "    \n",
    "- 欠損値を含む特徴量から別の特徴量を生成する際には、欠損値の置換に特に注意を払う必要がある。\n",
    "- また 数値変数の欠損を -999 などに補完してからTarget-Encoding などを行ってしまうと、補完した値に平均が引っ張られてしまう。\n",
    "    - このようなときは欠損値を無視して平均を取ったほうが良い。\n",
    "\n",
    "- XGBoost は欠損値をそのまま扱え、スコアを大幅に改善することがある。\n",
    "\n",
    "- 欠損値を異常値として扱うことができる。\n",
    "\n",
    "- 学習データに現れないカテゴリカル変数のカテゴリはモデルに欠損値として扱われるので、それは Frequency Encoding で置換して対処する方法が考えられる。\n",
    "    - 学習データとテストデータの両方を見て、その頻度を数え上げる。カテゴリの頻度と目的変数に依存関係があれば良い感じになる。\n",
    "        - データリークを引き起こすのでは？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リンク\n",
    "\n",
    "### Feature preprocessing\n",
    "\n",
    "- [Preprocessing in Sklearn](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Andrew NG about gradient descent and feature scaling\n",
    "](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling)\n",
    "- [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n",
    "### Feature generation\n",
    "\n",
    "- [Discover Feature Engineering, How to Engineer Features and How to Get Good at It\n",
    "](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "- [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction from Text and Images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text -> Vector\n",
    "\n",
    "- 1. Bag of words\n",
    "- 2. Enbeddings (~word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "### Bag of words\n",
    "- データから単語ごとに出現回数を数え上げて、文章ごとにそれらを列に配置する。\n",
    "- sklearn.feature_extranction.text.CountVectorizer で実装可能\n",
    "\n",
    "### TFiDF\n",
    "\n",
    "- スケーリングする後処理が必要である。\n",
    "- Term Frequency: 出現回数ではなく頻度（割合）を計算することで比較可能になる。\n",
    "    - ```tf = 1/x.sum(axis=1)[:,None]```\n",
    "    - ```x = x*tf```\n",
    "- Inverse Document Frequency: ドキュメントの逆数を掛けることで、あるドキュメントにのみ出現する珍しい単語を重視できる。\n",
    "    - ```idf = np.log(x.shape[0] / (x>0).sum(0)))```\n",
    "    - ```x = x*idf```\n",
    "- sklearn.feature_extraction.text.TfidfVectorizer で実装可能\n",
    "\n",
    "### N-grams\n",
    "\n",
    "- 単語ごとに特徴量として見なすのではなく、連続する単語ごとに特徴量として見なす。\n",
    "- unigrams, bigrams, trigrams\n",
    "- N-grams は単語数がWの場合、W**N この特徴列が生成されることになる。\n",
    "- sklearn.feature_extraction.text.CountVectorizer: *Ngram_range*, *analyzer*\n",
    "\n",
    "### Texts preprocessing\n",
    "\n",
    "- 1. Lowercase\n",
    "- 2. Lemmatization\n",
    "- 3. Stemming\n",
    "- 4. Stopwords\n",
    "\n",
    "#### 1. Lowercase\n",
    "\n",
    "- 小文字化: Sunny と sunny を同一のものとして扱いたいため\n",
    "- sklearn の confing ではデフォルトでこの処理を行ってくれる\n",
    "\n",
    "#### 2. Lemmatization\n",
    "\n",
    "- 例）: democracy, democratic, and democratization -> democracy\n",
    "- 事前知識（辞書）を用いて中心語を類推する\n",
    "\n",
    "#### 3. Stemming\n",
    "\n",
    "- 例）: democracy, democratic, and democratization -> democr\n",
    "- 語幹を抽出する\n",
    "\n",
    "#### 4. Stopwords\n",
    "\n",
    "- モデルにとって重要な情報を含まない単語\n",
    "    - 1. 重要ではない単語: 冠詞や前置詞\n",
    "    - 2. タスクの解決に役立たないようなよくある単語\n",
    "- NLTK (Natural Language ToolKit library for Python): ストップワードが予め定義されたリストを持つ\n",
    "- sklearn.feature_extraction.text.CountVectorizer: max_df（単語の閾値(?)を設定できる）\n",
    "\n",
    "### まとめ\n",
    "\n",
    "- BOW 適用のパイプライン\n",
    "    - 1. 前処理: 小文字化・ステミング・レンマ化・ストップワード除去\n",
    "    - 2. Ngrams: 局所的な文脈を捉えることに役立ちうる\n",
    "    - 3. 後処理: TFiDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec\n",
    "\n",
    "- Word2Vec は BOW と同様に単語や文章をベクトル化することを目指すが、もう少し正確に行う。\n",
    "- 異なる単語が同じ文脈で頻繁に用いられる場合、それらのベクトル空間上での距離は近いという発想を基本とする。\n",
    "    - king + woman - man = queen という有名な例のように、ベクトル化することで単語同士の足し算・引き算ができる。\n",
    "    - ベクトルの方向に意味を与えることができる。\n",
    "- 実装方法としては以下\n",
    "    - Words: Word2vec, Glove(Glocal Vector for word representation), FastText, etc.\n",
    "    - Sentences: Doc2vec, etc.\n",
    "        - 文章のベクトル化の方法には、単語ベクトルの平均・合計を計算する方法もある。\n",
    "    - 事前学習モデルが有る: Wikipedia\n",
    "    - ちなみに、Word2vec の学習にはラベルは不要で、各単語の文脈を抽出するためのテキストが必要なだけである。\n",
    "    \n",
    "### BOW と w2v の比較\n",
    "\n",
    "- 1. Bag of words\n",
    "    - a. 大きなベクトル\n",
    "    - b. ベクトルの各値の意味は解釈可能\n",
    "- 2. Word2vec\n",
    "    - a. 比較的小さなベクトル\n",
    "    - b. ベクトルの値はある場合にのみ解釈可能なことがある\n",
    "    - c. 同じような意味の単語はしばしば同じように埋め込みされる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image -> Vector\n",
    "\n",
    "- 1. Descriptors\n",
    "- 2. スクラッチでモデル構築\n",
    "- 3. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "### CNN\n",
    "- CNN は、単語に対して Word2vec で埋め込み表現を獲得するのと同様に、画像に対して圧縮された表現を獲得する。\n",
    "- 画像から得られた何らかの出力を Descriptors と呼ぶ\n",
    "\n",
    "### Fine-tuning\n",
    "- Descriptors の最初の層は入力に依存するところが大きいが、後の方ほど入力に依存しにくくなる。\n",
    "- （この性質を利用して、）事前学習モデルをタスクに適したものにチューニングすることがある。\n",
    "    - この処理を fine-tuning と呼ぶ。\n",
    "    - 特に小さなデータセットを使用する場合や同じようなタスクの場合、事前学習モデルを fine-tuning した方がスクラッチで学習したモデルよりも良い事が多い。\n",
    "- fine-tuning の例: 出力の最終層を除いて、タスクに適した出力層に変更する（その他の重みパラメータは変えない）\n",
    "\n",
    "### Augumentation\n",
    "- 学習データを増やしたいときに行なう。\n",
    "- 元の画像を回転（90度/180度/270度 etc.）させたりする。\n",
    "- モデルをロバストにし、過学習を避けるのに役立つ。\n",
    "- モデルを作る際に、学習データと検証データの分離に気をつける必要がある。\n",
    "\n",
    "### まとめ\n",
    "- a. 異なる層から特徴を抽出することができる。\n",
    "- b. 事前学習モデルの注意深い選択が役に立つことがある。\n",
    "- c. Fine-tuning は事前学習モデルを強化させることができる。\n",
    "- d. Data Augmentation はモデルを改善しうる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リンク\n",
    "\n",
    "### Feature extraction from text with Sklearn\n",
    "\n",
    "#### Bag of words\n",
    "- [Feature extraction from text with Sklearn](https://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "- [More examples of using Sklearn](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)\n",
    "#### Word2vec\n",
    "- [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/representation/word2vec)\n",
    "- [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)\n",
    "- [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)\n",
    "- [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)\n",
    "#### NLP Libraries\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "- [TextBlob](https://github.com/sloria/TextBlob)\n",
    "\n",
    "### Feature extraction from images\n",
    "#### Pretrained models\n",
    "- [Using pretrained models in Keras](https://keras.io/applications/)\n",
    "- [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)\n",
    "#### Finetuning\n",
    "- [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/hub/tutorials/image_retraining)\n",
    "- [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
