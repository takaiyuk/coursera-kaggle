{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction & Recap**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "\n",
    "- データコンペに関する説明\n",
    "- このコースの対象はMLの基礎事項を理解していることを前提としている。\n",
    "    - 具体的には、python, sklearn, MLモデルの学習についてある程度知っている。\n",
    "- 様々なプロセスで効率的に取り組めるようになるために、ある種の「直感」を養うことが必要。\n",
    "- コンペに時間を捧げるほど、より多くのことをを学ぶことができる。\n",
    "- このコースを提供しているのは、Kaggle 等のデータコンペに熱心に取り組んできた人物ら（ロシア・Yandex）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コース概要\n",
    "\n",
    "### スケジュール\n",
    "\n",
    "- **WEEK1: Starting Point**\n",
    "    - データコンペとRecapについて\n",
    "    - 特徴量の前処理と抽出\n",
    "- **WEEK2: Data Pipeline**\n",
    "    - EDA\n",
    "    - Validation\n",
    "    - Data Leaks\n",
    "- **WEEK3: Improvement**\n",
    "    - Metrics\n",
    "    - Mean-encodings\n",
    "- **WEEK4: Advanced**\n",
    "    - Advenced features\n",
    "    - Hyperparameter optimization\n",
    "    - Ensembles\n",
    "- **WEEK5: Wrapping Up**\n",
    "    - Final project\n",
    "    - Winning solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データコンペについて\n",
    "\n",
    "### 概要\n",
    "\n",
    "- ページ説明\n",
    "    - Data\n",
    "    - Model\n",
    "    - Submission\n",
    "    - Evaluation\n",
    "    - Leaderboard\n",
    "\n",
    "- プラットフォーム\n",
    "    - Kaggle\n",
    "    - DrivenData\n",
    "    - CrowdAnalityx\n",
    "    - CodaLab\n",
    "    - DataScienceChallenge.net\n",
    "    - Datascience.net\n",
    "    - Signle-competition sites(like KDD, VizDooM)\n",
    "    \n",
    "- 参加すべき理由\n",
    "    - 学習とネットワーキングの機会\n",
    "    - 様々なアプローチを試せる\n",
    "    - コミュニティに入れる\n",
    "    - 運が良ければ賞金を得られる\n",
    "    \n",
    "### 実世界とデータコンペの違い\n",
    "\n",
    "- 実世界\n",
    "    - ビジネス理解\n",
    "    - 問題の定式化\n",
    "    - データ収集\n",
    "    - データ前処理\n",
    "    - モデリング\n",
    "    - 評価指標を決定\n",
    "    - デプロイ\n",
    "- データコンペ\n",
    "    - （ビジネス理解）\n",
    "    - （データ収集）\n",
    "    - データ前処理\n",
    "    - モデリング\n",
    "    \n",
    "- データコンペはアルゴリズムのみではない\n",
    "    - 既存のアプローチをチューニングする必要がある\n",
    "        - 既存ライブラリのソースコードをいじったりすることも必要になりうる。\n",
    "    - 洞察が必要\n",
    "    - 時には ML 以外のことが必要になる\n",
    "        - ヒューリスティック・手動のデータ分析もOK\n",
    "        - 複雑な解放・先進的な特徴エンジニアリング・膨大な計算も避けなければならないわけではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "### ML アルゴリズムの系統\n",
    "\n",
    "#### Linear\n",
    "\n",
    "- アルゴリズム例\n",
    "    - Logistic Regression\n",
    "    - SVM\n",
    "- 特徴\n",
    "    - 線形モデルはスパースな高次元データに対して有効\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "    - vowpal-rabbit\n",
    "\n",
    "#### Tree-based\n",
    "\n",
    "- アルゴリズム例\n",
    "    - Desicion Tree\n",
    "    - Random Forest\n",
    "    - GBDT\n",
    "- 特徴\n",
    "    - 決定木の組み合わせによって表現力が高く、テーブルデータに対して有効\n",
    "    - 線形従属（斜めの線?）を捉えるのは苦手である。\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "    - dmlc/XGBoost\n",
    "    - Microsoft/LightGBM\n",
    "    \n",
    "#### kNN\n",
    "\n",
    "- 特徴\n",
    "    - 距離が近いものが似たラベルを持つという発想。\n",
    "    - シンプルな方法だが、近傍法に基づく特徴量は有益なことがしばしばある。\n",
    "- パッケージ\n",
    "    - sklearn\n",
    "\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "- 特徴\n",
    "    - 決定木系と比べて滑らかな分離カーブを表現できる\n",
    "    - 画像・音声・テキスト・文章に特に有効なことがある。\n",
    "- フレームワーク\n",
    "    - TensorFlow\n",
    "    - Keras\n",
    "    - MXNet\n",
    "    - PyTorch\n",
    "    - Lasagne\n",
    "    \n",
    "### No Free Lunch Theorem\n",
    "\n",
    "- 全てのタスクにおいて他の全てのモデルを凌ぐモデルは存在しないということ。\n",
    "- そのため様々なモデルに精通している必要がある。\n",
    "\n",
    "### Scikit-Learn の分離器比較\n",
    "\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "- 様々な分離器の決定境界を概観できる。\n",
    "- なぜそのような決定境界ができるのか、その理由を直感的に理解できることが推奨される。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "- 「銀の弾丸」のアルゴリズムは存在しない\n",
    "- 線形モデルは空間を2つの小空間に分ける\n",
    "- ツリーベースモデルは空間を箱に分ける\n",
    "- kNNモデルはどのように各点の「近さ」を定義するかに大きく依存する\n",
    "- 順伝播ニューラルネットは滑らかな非線形の決定境界を生成する\n",
    "- 最も強力な手法は GDBT と NN だが、線形モデルやkNNも時にそれらを凌ぐことがあるので過小評価してはならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Preprocessing and Generation with Respect to Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## イントロダクション\n",
    "\n",
    "### 前処理と特徴量生成について\n",
    "\n",
    "前処理も特徴量生成もどのように処理するかは用いるモデルタイプに依存する。\n",
    "以下、具体例を示す。\n",
    "\n",
    "- カテゴリカル変数の前処理\n",
    "    - 線形モデルは One-hot Encoding などが有効\n",
    "    - ツリーベースモデルでは不要\n",
    "    \n",
    "- トレンドを反映させるための特徴量生成\n",
    "    - 線形モデルでは前週の売上を特徴量として足す\n",
    "    - ツリーベースでは Window で Target Encoding したものを特徴量として足す\n",
    "    \n",
    "## 数値変数（Numerical Features）\n",
    "\n",
    "### 前処理\n",
    "\n",
    "#### スケーリング（Scaling）\n",
    "\n",
    "- ツリーベースモデル\n",
    "    - 値のスケールに影響されない\n",
    "- 非ツリーベースモデル\n",
    "    - 値のスケールに影響を受ける\n",
    "    - kNN の例にして考えるとスケールが予測精度に与える影響を理解しやすい\n",
    "    \n",
    "- 具体的な手法\n",
    "    - sklearn.preprocessing.MinMaxScaler()\n",
    "        - 指定した最小値と最大値の範囲に値をスケーリングする\n",
    "    - sklearn.preprocessing.StandardScaler()\n",
    "        - mean を引いて std で割る（平均0・標準偏差を1にする）\n",
    "        \n",
    "- kNN においてスケーリングの考え方を応用して、より重要だと思われる特徴量のスケールを大きくすることで、モデルへのその特徴量の影響度合いを（意図的に）大きくすることができる。\n",
    "\n",
    "#### 外れ値（Outliers）\n",
    "\n",
    "- 線形モデルは外れ値に影響を受けやすい\n",
    "- これを避けるためのアプローチがいくつかある。\n",
    "    - 1. パーセンタイルをもとに値をクリッピングする\n",
    "    - 2. ランク変換（Rank Transformation）を行う。値のマッピングを行うイメージか。\n",
    "        - ```rank([-100, 0, 1e5])==[0, 1, 2]```\n",
    "        - ```rank([1000, 1, 10]) = [2, 0, 1]```\n",
    "        \n",
    "#### その他\n",
    "\n",
    "- 非ツリーベースモデル（特にNN）でしばしば有効な前処理\n",
    "    - ログ変換（Log Transform): np.log(1+x)\n",
    "    - 指数変換(Rasing to the power<1): np.sqrt(x + 2/3)\n",
    "- これらは過分散を抑制する一方で、0に近い値の差を大きくする\n",
    "- 様々な前処理を行って学習を行ったモデルを組み合わせることはとても有効になりうる。\n",
    "\n",
    "### 特徴量生成\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
